{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDD 3Hz Backbone Notebook\n",
    "End-to-end pipeline: scan -> split -> index -> normalize -> train encoder -> (future) evaluation.\n",
    "\n",
    "This notebook orchestrates the workflow while keeping heavy logic in .py modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a4418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from hdd_dataset import (\n",
    "    scan_dataset,\n",
    "    build_session_splits,\n",
    "    build_window_index,\n",
    "    compute_normalization,\n",
    "    HDDWindowDataset,\n",
    "    CHANNEL_NAMES,\n",
    "    FS_HZ,\n",
    "    DEFAULT_WINDOW,\n",
    "    DEFAULT_HOP_TRAIN,\n",
    "    DEFAULT_HOP_INFER,\n",
    ")\n",
    "import train_encoder\n",
    "import extract_embeddings\n",
    "from models import build_model\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "config = {\n",
    "    'model_name': 'tcn_ae',\n",
    "    'train_index': 'artifacts/index_train_sparse.jsonl',\n",
    "    'val_index': 'artifacts/index_val_sparse.jsonl',\n",
    "    'sensor_dir': '20200710_sensors/sensor',\n",
    "    'label_dir': '20200710_labels/target',\n",
    "    'normalization': 'artifacts/normalization.json',\n",
    "    'window': DEFAULT_WINDOW,\n",
    "    'embedding_dim': 20,\n",
    "    'hidden_channels': 32,\n",
    "    'num_layers': 3,\n",
    "    'kernel_size': 3,\n",
    "    'dropout': 0.0,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 50,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'max_steps_per_epoch': 0,\n",
    "    'num_workers': 0,\n",
    "    'cache_size': 32,\n",
    "    'seed': 123,\n",
    "    'device': 'auto',\n",
    "    'amp': False,\n",
    "    'out_dir': 'artifacts/encoder',\n",
    "}\n",
    "\n",
    "set_seed(config['seed'])\n",
    "\n",
    "artifacts_dir = Path('artifacts')\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "sensor_dir = Path(config['sensor_dir'])\n",
    "label_dir = Path(config['label_dir'])\n",
    "window = int(config['window'])\n",
    "hop_sparse = DEFAULT_HOP_TRAIN\n",
    "hop_dense = DEFAULT_HOP_INFER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c24bc258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions with sensors+labels: 137\n",
      "Lengths (frames): min 792, max 30495, mean 8199.96\n",
      "Total duration: 6241.08 minutes\n",
      "Label inventory: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Summary saved to artifacts\\scan_summary.json\n"
     ]
    }
   ],
   "source": [
    "# 1) Dataset scan and validation\n",
    "summary = scan_dataset(sensor_dir, label_dir, window=window)\n",
    "scan_path = artifacts_dir / 'scan_summary.json'\n",
    "scan_path.write_text(json.dumps(summary, indent=2))\n",
    "print(f'Summary saved to {scan_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91175fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits saved to artifacts\\splits.json\n",
      "{'train': 95, 'val': 20, 'test': 22}\n"
     ]
    }
   ],
   "source": [
    "# 2) Session-level splits (by session_id only)\n",
    "session_ids = sorted(summary['per_session_label_counts'].keys())\n",
    "splits = build_session_splits(session_ids, seed=config['seed'])\n",
    "split_path = artifacts_dir / 'splits.json'\n",
    "split_path.write_text(json.dumps(splits, indent=2))\n",
    "print(f'Splits saved to {split_path}')\n",
    "print({k: len(v) for k, v in splits.items() if k != 'seed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193d8370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: sparse=254334 dense=762894\n",
      "val: sparse=76350 dense=229029\n",
      "test: sparse=43055 dense=129143\n"
     ]
    }
   ],
   "source": [
    "# 3) Build window indices (sparse=hop=3, dense=hop=1)\n",
    "index_paths = {}\n",
    "for split_name, ids in splits.items():\n",
    "    if split_name == 'seed':\n",
    "        continue\n",
    "    idx_sparse = build_window_index(ids, sensor_dir, window=window, hop=hop_sparse)\n",
    "    idx_dense = build_window_index(ids, sensor_dir, window=window, hop=hop_dense)\n",
    "    sparse_path = artifacts_dir / f'index_{split_name}_sparse.jsonl'\n",
    "    dense_path = artifacts_dir / f'index_{split_name}_dense.jsonl'\n",
    "    with sparse_path.open('w', encoding='utf-8') as f:\n",
    "        for rec in idx_sparse:\n",
    "            f.write(json.dumps(rec) + '\\n')\n",
    "    with dense_path.open('w', encoding='utf-8') as f:\n",
    "        for rec in idx_dense:\n",
    "            f.write(json.dumps(rec) + '\\n')\n",
    "    index_paths[split_name] = {'sparse': sparse_path, 'dense': dense_path}\n",
    "    print(f\"{split_name}: sparse={len(idx_sparse)} dense={len(idx_dense)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "072e4ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization saved to artifacts\\normalization.json\n",
      "binary_channels: ['lturn', 'rturn']\n",
      "non_normalized_channels: ['lturn', 'rturn']\n"
     ]
    }
   ],
   "source": [
    "# 4) Normalization (train sessions only)\n",
    "normalization = compute_normalization(splits['train'], sensor_dir)\n",
    "norm_path = artifacts_dir / 'normalization.json'\n",
    "norm_path.write_text(json.dumps(normalization, indent=2))\n",
    "print(f'Normalization saved to {norm_path}')\n",
    "print('binary_channels:', normalization.get('binary_channels'))\n",
    "print('non_normalized_channels:', normalization.get('non_normalized_channels'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94883e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Train encoder (self-supervised)\n",
    "train_result = train_encoder.train(config)\n",
    "print('Best checkpoint:', train_result['best_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Sanity recon + embedding shape\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and config['device'] != 'cpu' else 'cpu')\n",
    "model = build_model(\n",
    "    config['model_name'],\n",
    "    in_channels=8,\n",
    "    latent_dim=config['embedding_dim'],\n",
    "    hidden_channels=config['hidden_channels'],\n",
    "    num_layers=config['num_layers'],\n",
    "    kernel_size=config['kernel_size'],\n",
    "    dropout=config['dropout'],\n",
    ")\n",
    "state = torch.load(train_result['best_path'], map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "ds = HDDWindowDataset(\n",
    "    index_paths['val']['sparse'],\n",
    "    sensor_dir=sensor_dir,\n",
    "    label_dir=None,\n",
    "    window=window,\n",
    "    normalization=normalization,\n",
    "    return_label=False,\n",
    "    cache_size=8,\n",
    "    to_tensor=True,\n",
    ")\n",
    "x, meta = ds[0]\n",
    "x = x.unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    recon, z = model(x)\n",
    "print('x shape:', x.shape, 'recon:', recon.shape, 'z:', z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad8787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Plot one reconstruction (input vs recon)\n",
    "import matplotlib.pyplot as plt\n",
    "x_np = x.squeeze(0).cpu().numpy()\n",
    "r_np = recon.squeeze(0).cpu().numpy()\n",
    "time_axis = np.arange(x_np.shape[0]) / FS_HZ\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10, 8), sharex=True)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.plot(time_axis, x_np[:, i], label='input')\n",
    "    ax.plot(time_axis, r_np[:, i], label='recon', alpha=0.7)\n",
    "    ax.set_title(CHANNEL_NAMES[i])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "axes[-1, 0].set_xlabel('Time (s)')\n",
    "axes[-1, 1].set_xlabel('Time (s)')\n",
    "axes[0, 0].legend(loc='upper right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Evaluation stub (embeddings extraction)\n",
    "RUN_EXTRACTION = False\n",
    "if RUN_EXTRACTION:\n",
    "    extract_config = {\n",
    "        'splits': 'train,val,test',\n",
    "        'index_dir': 'artifacts',\n",
    "        'sensor_dir': config['sensor_dir'],\n",
    "        'normalization': config['normalization'],\n",
    "        'weights': train_result['best_path'],\n",
    "        'config': str(Path(config['out_dir']) / 'config.json'),\n",
    "        'out_dir': 'artifacts/embeddings',\n",
    "        'batch_size': 256,\n",
    "        'num_workers': 0,\n",
    "        'device': config['device'],\n",
    "    }\n",
    "    extract_embeddings.extract(extract_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d4a0316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinism check max|z1-z2|: 0.000000e+00\n",
      "Z shape: (762894, 20)\n",
      "Z mean min/max: -0.4176/0.2570\n",
      "Z std  min/max: 0.3988/1.0473\n",
      "Fraction of near-zero std dims: 0.0000\n",
      "Determinism check max|z1-z2|: 0.000000e+00\n",
      "Z shape: (229029, 20)\n",
      "Z mean min/max: -0.4269/0.2722\n",
      "Z std  min/max: 0.3954/1.0442\n",
      "Fraction of near-zero std dims: 0.0000\n",
      "Determinism check max|z1-z2|: 0.000000e+00\n",
      "Z shape: (129143, 20)\n",
      "Z mean min/max: -0.3881/0.1698\n",
      "Z std  min/max: 0.3927/0.9886\n",
      "Fraction of near-zero std dims: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 9) Latent PCA health check (per-split)\n",
    "RUN_PCA = True #False by default\n",
    "if RUN_PCA:\n",
    "    import sys\n",
    "    import importlib\n",
    "    import scripts.latent_pca_viz as latent_pca_viz\n",
    "    importlib.reload(latent_pca_viz)\n",
    "    from scripts.latent_pca_viz import main as latent_pca_main\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        sys.argv = [\n",
    "            \"latent_pca_viz.py\",\n",
    "            \"--ckpt\", \"artifacts/encoder/weights_best.pt\",\n",
    "            \"--split\", split,\n",
    "            \"--batch_size\", \"256\",\n",
    "            \"--seed\", \"123\",\n",
    "            \"--out_dir\", \"artifacts/latent_viz\",\n",
    "            \"--out_npz\", f\"artifacts/latent_viz/{split}_pca.npz\",\n",
    "        ]\n",
    "        latent_pca_main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50e4e0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated PCA outputs: artifacts\\latent_viz\\train_pca.npz\n",
      "Updated PCA outputs: artifacts\\latent_viz\\val_pca.npz\n",
      "Updated PCA outputs: artifacts\\latent_viz\\test_pca.npz\n"
     ]
    }
   ],
   "source": [
    "# 9b) Align PCA basis (train -> val/test)\n",
    "RUN_ALIGN_PCA = True #False by default\n",
    "if RUN_ALIGN_PCA:\n",
    "    import sys\n",
    "    import runpy\n",
    "\n",
    "    sys.argv = [\n",
    "        \"scripts/align_pca_basis.py\",\n",
    "        \"--train_npz\", \"artifacts/latent_viz/train_pca.npz\",\n",
    "        \"--val_npz\", \"artifacts/latent_viz/val_pca.npz\",\n",
    "        \"--test_npz\", \"artifacts/latent_viz/test_pca.npz\",\n",
    "        \"--pca_dim\", \"6\",\n",
    "    ]\n",
    "    runpy.run_path(\"scripts/align_pca_basis.py\", run_name=\"__main__\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22dfe8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hopkins statistic: 0.9686 (near 1 -> highly clusterable)\n",
      "\n",
      "Top correlations per PC:\n",
      "PC1: brake_mean=-0.819, accel_pedal_mean=0.794, speed_mean=0.745\n",
      "PC2: steer_angle_mean=-0.862, yaw_mean=-0.826, rturn_frac=-0.328\n",
      "PC3: speed_mean=0.497, abs_yaw=-0.293, brake_mean=0.249\n",
      "PC4: steer_speed_mean=0.693, yaw_mean=-0.354, speed_mean=-0.124\n",
      "PC5: steer_angle_mean=0.362, steer_speed_mean=-0.292, yaw_mean=-0.225\n",
      "PC6: steer_angle_mean=-0.302, yaw_mean=0.216, rturn_frac=0.164\n",
      "\n",
      "Silhouette (best K):\n",
      "K=12, silhouette=0.2985\n",
      "\n",
      "Stability summary (ARI):\n",
      "K=8: mean=0.615, min=0.474, max=0.885\n",
      "K=12: mean=0.627, min=0.501, max=0.709\n",
      "K=16: mean=0.605, min=0.475, max=0.705\n",
      "K=20: mean=0.555, min=0.451, max=0.723\n"
     ]
    }
   ],
   "source": [
    "# 10) Latent diagnostics (terminal-free)\n",
    "RUN_DIAGNOSTICS = True #False by default\n",
    "if RUN_DIAGNOSTICS:\n",
    "    import sys\n",
    "    import runpy\n",
    "\n",
    "    sys.argv = [\n",
    "        \"scripts/latent_diagnostics.py\",\n",
    "        \"--pca_npz\", \"artifacts/latent_viz/train_pca.npz\",\n",
    "        \"--use_space\", \"pc_scores\",\n",
    "        \"--max_pc\", \"6\",\n",
    "        \"--k_min\", \"4\",\n",
    "        \"--k_max\", \"40\",\n",
    "        \"--k_step\", \"4\",\n",
    "        \"--stability_k\", \"8\", \"12\", \"16\", \"20\",\n",
    "    ]\n",
    "    runpy.run_path(\"scripts/latent_diagnostics.py\", run_name=\"__main__\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d9b15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 11) Stage 2: KMeans clustering in PCA space + smoothing\n",
    "# RUN_STAGE2 = True #False by default\n",
    "# if RUN_STAGE2:\n",
    "#     import stage2.kmeans_cluster as km\n",
    "#     km.main([\n",
    "#         \"--pca_npz_train\", \"artifacts/latent_viz/train_pca.npz\",\n",
    "#         \"--pca_npz_val\", \"artifacts/latent_viz/val_pca.npz\",\n",
    "#         \"--pca_npz_test\", \"artifacts/latent_viz/test_pca.npz\",\n",
    "#         \"--pca_dim\", \"6\",\n",
    "#         \"--k\", \"16\",\n",
    "#         \"--smooth_window\", \"5\",\n",
    "#         \"--seed\", \"123\",\n",
    "#         \"--out_dir\", \"artifacts/stage2\",\n",
    "#         \"--splits\", \"train,val,test\",\n",
    "#         \"--viz_all\",\n",
    "#         \"--viz_label\", \"smooth\",\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9177f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Stage 2: KMeans clustering in PCA space + smoothing\n",
    "RUN_STAGE2 = True #False by default\n",
    "if RUN_STAGE2:\n",
    "    import importlib\n",
    "    import stage2.kmeans_cluster as km\n",
    "    importlib.reload(km)\n",
    "\n",
    "    km.main([\n",
    "        \"--pca_npz_train\", \"artifacts/latent_viz/train_pca.npz\",\n",
    "        \"--pca_npz_val\", \"artifacts/latent_viz/val_pca.npz\",\n",
    "        \"--pca_npz_test\", \"artifacts/latent_viz/test_pca.npz\",\n",
    "        \"--pca_dim\", \"6\",\n",
    "        \"--k\", \"16\",\n",
    "        \"--smooth_window\", \"5\",\n",
    "        \"--seed\", \"123\",\n",
    "        \"--out_dir\", \"artifacts/stage2\",\n",
    "        \"--splits\", \"train,val,test\",\n",
    "        \"--viz_all\",\n",
    "        \"--viz_label\", \"smooth\",\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed2fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
